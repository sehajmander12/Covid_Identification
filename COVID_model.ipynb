{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"dataset/train\"\n",
    "val_path = \"dataset/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda navigator\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda navigator\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 4 convolution layers\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation = 'relu',input_shape = (224,224,3)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation = 'relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = keras.losses.binary_crossentropy,optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 220, 220, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 110, 110, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 110, 110, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 108, 108, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                5537856   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,668,097\n",
      "Trainable params: 5,668,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "train_datagen = image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "\n",
    ")\n",
    "\n",
    "test_dataset = image.ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = train_datagen.flow_from_directory(\n",
    "    'dataset/train',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covid_p': 0, 'normal': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_gen = test_dataset.flow_from_directory(\n",
    "    'dataset/val',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda navigator\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 67s 7s/step - loss: 0.9408 - accuracy: 0.5362 - val_loss: 0.7172 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 71s 7s/step - loss: 0.6656 - accuracy: 0.5641 - val_loss: 0.6187 - val_accuracy: 0.8125\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 69s 7s/step - loss: 0.4935 - accuracy: 0.7730 - val_loss: 0.3116 - val_accuracy: 0.8333\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 65s 7s/step - loss: 0.3078 - accuracy: 0.8846 - val_loss: 0.2212 - val_accuracy: 0.8958\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 69s 7s/step - loss: 0.3008 - accuracy: 0.9054 - val_loss: 0.2118 - val_accuracy: 0.9583\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 73s 7s/step - loss: 0.2131 - accuracy: 0.9199 - val_loss: 0.1062 - val_accuracy: 0.9375\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 73s 7s/step - loss: 0.2260 - accuracy: 0.9263 - val_loss: 0.2447 - val_accuracy: 0.9375\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 72s 7s/step - loss: 0.1748 - accuracy: 0.9441 - val_loss: 0.0766 - val_accuracy: 0.9792\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 69s 7s/step - loss: 0.1741 - accuracy: 0.9441 - val_loss: 0.1190 - val_accuracy: 0.9792\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 68s 7s/step - loss: 0.2214 - accuracy: 0.9539 - val_loss: 0.1918 - val_accuracy: 0.9583\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 73s 7s/step - loss: 0.2139 - accuracy: 0.9391 - val_loss: 0.1889 - val_accuracy: 0.9792\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 68s 7s/step - loss: 0.1782 - accuracy: 0.9507 - val_loss: 0.1564 - val_accuracy: 0.9792\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 68s 7s/step - loss: 0.1597 - accuracy: 0.9474 - val_loss: 0.1243 - val_accuracy: 0.9792\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 75s 7s/step - loss: 0.1380 - accuracy: 0.9507 - val_loss: 0.1636 - val_accuracy: 0.9792\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 73s 7s/step - loss: 0.1634 - accuracy: 0.9487 - val_loss: 0.0870 - val_accuracy: 0.9375\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 70s 7s/step - loss: 0.1790 - accuracy: 0.9441 - val_loss: 0.1613 - val_accuracy: 0.8750\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 72s 7s/step - loss: 0.2525 - accuracy: 0.9295 - val_loss: 0.0658 - val_accuracy: 0.9792\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 69s 7s/step - loss: 0.1429 - accuracy: 0.9605 - val_loss: 0.0055 - val_accuracy: 0.9583\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 72s 7s/step - loss: 0.1468 - accuracy: 0.9605 - val_loss: 0.0144 - val_accuracy: 0.9792\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 74s 7s/step - loss: 0.1211 - accuracy: 0.9647 - val_loss: 0.0621 - val_accuracy: 0.9583\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=10,\n",
    "    epochs = 20,\n",
    "    validation_data = val_gen,\n",
    "    validation_steps = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Covid_detection.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07642696052789688, 0.97826087474823]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17409595847129822, 0.9583333134651184]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covid_p': 0, 'normal': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model = load_model(\"Covid_detection.h5\")\n",
    "train_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "y_test= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"./dataset/val/normal/\"):\n",
    "    img = image.load_img(\"./dataset/val/normal/\" + i, target_size = (224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis= 0)\n",
    "    pred = model.predict_classes(img)\n",
    "    y_test.append(pred[0,0])\n",
    "    y_actual.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"./dataset/val/covid_p/\"):\n",
    "    img = image.load_img(\"./dataset/val/covid_p/\" + i, target_size = (224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis= 0)\n",
    "    pred = model.predict_classes(img)\n",
    "    y_test.append(pred[0,0])\n",
    "    y_actual.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_actual)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_actual)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_actual,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 0.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sn.heatmap(matrix,cmap = \"Blues\", annot = True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
